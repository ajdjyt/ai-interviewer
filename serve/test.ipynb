{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave2Vec2.0 Test for sexy TTS later perhaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU datasets\n",
    "%pip install -qU transformers\n",
    "%pip install -qU torchaudio\n",
    "%pip install -qU jiwer\n",
    "%pip install -qU accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "torch.__version__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_16_0\", \"en\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_16_0\", \"en\", split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU python-dotenv sounddevice ffmpeg-python requests groq gtts numpy torch torchaudio silero-vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "import torch\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import requests\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "from groq import Groq\n",
    "import numpy as np\n",
    "import sys\n",
    "import queue\n",
    "import threading\n",
    "import wave\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "groq = Groq(api_key=GROQ_API_KEY)\n",
    "vad = load_silero_vad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "    tts.save(temp_file.name)\n",
    "    os.system(f\"ffplay -nodisp -autoexit {temp_file.name}\") \n",
    "    os.remove(temp_file.name)\n",
    "\n",
    "# STT: send audio to Groq Whisper API for transcription\n",
    "def send_to_groq_whisper_api(\n",
    "    filename=\"./recording.wav\", \n",
    "    api_url=\"https://api.groq.com/openai/v1/audio/transcriptions\", \n",
    "    api_key=GROQ_API_KEY\n",
    "):\n",
    "    print(\"Sending audio to Groq Whisper API...\")\n",
    "    \n",
    "    with open(filename, 'rb') as audio_file:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        files = {\n",
    "            \"file\": (os.path.basename(filename), audio_file, \"audio/wav\"),\n",
    "            \"model\": (None, \"whisper-large-v3-turbo\") \n",
    "        }\n",
    "        \n",
    "        response = requests.post(api_url, headers=headers, files=files)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Transcription successful!\")\n",
    "        return response.json().get(\"text\", \"\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_chunk(vad_model, audio_chunk, sample_rate):\n",
    "    audio_chunk = np.array(audio_chunk, dtype=np.float32)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:\n",
    "        temp_filename = temp_file.name\n",
    "        \n",
    "        with wave.open(temp_filename, 'wb') as wf:\n",
    "            wf.setnchannels(1) \n",
    "            wf.setsampwidth(2)\n",
    "            wf.setframerate(sample_rate)\n",
    "            wf.writeframes(audio_chunk.tobytes())\n",
    "            \n",
    "        wav = read_audio(temp_filename)\n",
    "        speech_timestamps = get_speech_timestamps(wav, vad_model, return_seconds=True, sampling_rate=sample_rate)\n",
    "\n",
    "    print(\"Speech Timestamps:\", speech_timestamps)\n",
    "    return speech_timestamps\n",
    "\n",
    "def silero_real_time(file_name=\"./recording.wav\", sample_rate=16000, chunk_duration=2, threshold_silence=2.0):\n",
    "    vad_model = load_silero_vad()\n",
    "    audio_queue = queue.Queue()\n",
    "    recorded_audio = []\n",
    "    silence_duration = 0\n",
    "    \n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            print(status, file=sys.stderr)\n",
    "        print(f\"Callback triggered, frames: {frames} Audio chunk: {indata[:3]}...\")\n",
    "        audio_queue.put(indata)\n",
    "\n",
    "    # Start recording in a non-blocking way\n",
    "    with sd.InputStream(callback=callback, channels=1, samplerate=sample_rate, blocksize=chunk_duration * sample_rate):\n",
    "        print(\"Start speaking...\")\n",
    "        recording = False\n",
    "        \n",
    "        while True:\n",
    "            audio_chunk = audio_queue.get()\n",
    "            speech_timestamps = process_audio_chunk(vad_model, audio_chunk, sample_rate)\n",
    "            \n",
    "            if speech_timestamps:\n",
    "                silence_duration = 0\n",
    "                if not recording:\n",
    "                    print(\"Speech detected. Starting recording...\")\n",
    "                    recording = True\n",
    "                recorded_audio.append(audio_chunk)\n",
    "                \n",
    "            else:\n",
    "                silence_duration += chunk_duration\n",
    "                print(f\"Silence:{silence_duration}\")\n",
    "                if silence_duration >= threshold_silence:\n",
    "                    print(\"Silence detected for long enough. Stopping recording...\")\n",
    "                    break\n",
    "\n",
    "    if recorded_audio:\n",
    "        final_audio = np.concatenate(recorded_audio, axis=0)\n",
    "        sd.write(file_name, final_audio, sample_rate)\n",
    "        print(f\"Recording saved as '{file_name}'\")\n",
    "        return file_name\n",
    "    else:\n",
    "        print(\"No speech detected during recording.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start speaking...\n",
      "Callback triggered, frames: 32000 Audio chunk: [[-0.00446457]\n",
      " [-0.00704147]\n",
      " [-0.00496611]]...\n",
      "Speech Timestamps: []\n",
      "Silence:2\n",
      "Silence detected for long enough. Stopping recording...\n",
      "Callback triggered, frames: 32000 Audio chunk: [[0.0114101 ]\n",
      " [0.01295299]\n",
      " [0.01304272]]...\n",
      "No speech detected during recording.\n"
     ]
    }
   ],
   "source": [
    "file = silero_real_time()\n",
    "if file:\n",
    "    transcription = send_to_groq_whisper_api(filename=file)\n",
    "    if transcription:\n",
    "        print(\"Transcription Result:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate interview questions using t2t LLM\n",
    "def generate_question(job_description, prompt=\"\", prev=None):\n",
    "    system_message = f\"\"\"Based on the job description: {job_description}\\n\n",
    "        You are conducting an interview, acting as an expert on the topics mentioned in the job description\n",
    "        End the interview when necessary\n",
    "        Your response is directly relayed to the user, Only respond with the interview question itself, without any introductions or extra phrases.\n",
    "        \"\"\"\n",
    "    if prev:\n",
    "        prompt += f\"Candidate: {prev}\\n\"\n",
    "    prompt += \"Please generate the next interview question:\"\n",
    "    \n",
    "    response = groq.chat.completions.create(\n",
    "        model=\"llama-3.2-90b-text-preview\",\n",
    "        max_tokens=150,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_message\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response['text'].strip(), prompt\n",
    "\n",
    "# Interview Loop\n",
    "def start_interview(job_description):\n",
    "    prompt=\"Interview start\\n\"\n",
    "    text_to_speech(\"Welcome to the interview. Let's get started!\")\n",
    "    question, prompt = generate_question(job_description, prompt)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        text_to_speech(question)\n",
    "        print(\"Recording your answer...\")\n",
    "        \n",
    "        record_audio_with_vad()\n",
    "        \n",
    "        answer = send_to_groq_whisper_api(filename=\"./recording.wav\")\n",
    "        print(f\"Transcribed Answer: {answer}\")\n",
    "        \n",
    "        question, prompt = generate_question(job_description, prompt, prev=answer)\n",
    "        prompt += f\"Interviewer: {question}\\n\"\n",
    "        \n",
    "        if \"end interview\" in question.lower():\n",
    "            text_to_speech(\"Thank you for participating in the interview.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version n7.1 Copyright (c) 2003-2024 the FFmpeg developers\n",
      "  built with gcc 14.2.1 (GCC) 20240910\n",
      "  configuration: --prefix=/usr --disable-debug --disable-static --disable-stripping --enable-amf --enable-avisynth --enable-cuda-llvm --enable-lto --enable-fontconfig --enable-frei0r --enable-gmp --enable-gnutls --enable-gpl --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libdav1d --enable-libdrm --enable-libdvdnav --enable-libdvdread --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgsm --enable-libharfbuzz --enable-libiec61883 --enable-libjack --enable-libjxl --enable-libmodplug --enable-libmp3lame --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libplacebo --enable-libpulse --enable-librav1e --enable-librsvg --enable-librubberband --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libv4l2 --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpl --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-nvdec --enable-nvenc --enable-opencl --enable-opengl --enable-shared --enable-vapoursynth --enable-version3 --enable-vulkan\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[mp3 @ 0x79196c000c80] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, mp3, from '/tmp/tmpcvhjykd9.mp3':\n",
      "  Duration: 00:00:03.38, start: 0.000000, bitrate: 64 kb/s\n",
      "  Stream #0:0: Audio: mp3 (mp3float), 24000 Hz, mono, fltp, 64 kb/s\n",
      "   3.27 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletion' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start the interview with a given job description\u001b[39;00m\n\u001b[1;32m      2\u001b[0m job_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCloud Intern with expertise in Docker.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstart_interview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36mstart_interview\u001b[0;34m(job_description)\u001b[0m\n\u001b[1;32m     30\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterview start\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m text_to_speech(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWelcome to the interview. Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms get started!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m question, prompt \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     text_to_speech(question)\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mgenerate_question\u001b[0;34m(job_description, prompt, prev)\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease generate the next interview question:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m groq\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.2-90b-text-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip(), prompt\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start the interview with a given job description\n",
    "job_description = \"Cloud Intern with expertise in Docker.\"\n",
    "start_interview(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install webrtcvad\n",
    "\n",
    "import wave\n",
    "import webrtcvad\n",
    "\n",
    "def record_audio_with_vad(output_filename=\"./recording.wav\", sample_rate=16000, duration=10, vad_aggressiveness=3):\n",
    "\n",
    "    print(\"Recording with VAD...\")\n",
    "\n",
    "    # Initialize VAD\n",
    "    vad = webrtcvad.Vad(vad_aggressiveness)\n",
    "    buffer_duration = 0.03  # 30 ms\n",
    "    buffer_size = int(sample_rate * buffer_duration)\n",
    "    \n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "\n",
    "    audio_bytes = audio.tobytes()\n",
    "\n",
    "    # Process audio in chunks and perform VAD\n",
    "    speech_detected = False\n",
    "    speech_frames = []\n",
    "    for i in range(0, len(audio_bytes), buffer_size * 2):  # 2 bytes per int16 sample\n",
    "        frame = audio_bytes[i:i + buffer_size * 2]\n",
    "        if len(frame) < buffer_size * 2:\n",
    "            break\n",
    "\n",
    "        if vad.is_speech(frame, sample_rate):\n",
    "            speech_detected = True\n",
    "            speech_frames.append(frame)\n",
    "        elif speech_detected:\n",
    "            # Stop when speech ends\n",
    "            break\n",
    "\n",
    "    with wave.open(output_filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)  # 2 bytes for int16\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(speech_frames))\n",
    "\n",
    "    print(f\"Recording saved to {output_filename}\")\n",
    "    return output_filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
